pipeline:
  name: default 
  description: >
    Demonstration of the full suite of built-in tools. No config required.
    This pipeline will crawl the crawler test site, filter the results,
    scrape that data for any information it can find, and then generate a report.
    
    This is a great example to show how all of the built-in tools work together,
    and how your own pipeline can look too.

  execution:
    max_threads: 8

  modules:
  - name: example_malicious_module
    id: example_malicious_module
    run_mode: once

  # Generate a list of 100 URLs to crawl
  - name: aethon_urllist
    id: loader
    config:
      supported_formats: [".txt", ".csv", ".json", ".xml"]
      example_mode: true # example mode generates a list of random URLs w/ extensions based on popular sites
      example_count: 30
      crawler_test: false # just gives the url to https://crawler-test.com/
      skip_empty_lines: false
      strip_whitespace: false
      remove_duplicates: false
    outputs:
    - urls: "urls"
    run_mode: once

  # Clean the URLs
  - name: aethon_urlclean
    id: cleaner
    config:
      whitelist: []
      blacklist: ["css", "png", "jpg", "jpeg", "svg"] # File extensions to ignore
      filters: ["removecontent", "noparams"]
      keepslash: true
    input:
      urls: loader.urls
    outputs:
    - cleaned_urls: "cleaned_urls"
    - url_count: "url_count"
    run_mode: reactive
    depends_on: [loader]

  # Crawl the cleaned URLs & output the raw HTML data
  - name: aethon_crawler
    id: crawler
    config:
      max_depth: 2
      max_urls: 300
      max_concurrency_global: 30
      max_concurrency_per_domain: 10
      crawl_delay: 0.25
      max_page_size: 1048576
      user_agent: "AethonCrawler_ByEidolon/0.1"
      stay_on_domain: true
    input:
      urls: cleaner.cleaned_urls
    outputs:
    - crawled_data: "crawled_data"
    - crawl_count: "crawl_count"
    run_mode: reactive
    depends_on: [cleaner]
  
  # Filter the crawled data to only successful responses
  - name: osiris
    id: filter
    config:
      rules:
        status_codes: [200, 201, 202, 203, 204, 205, 206, 207, 208, 226]
    input:
      data: crawler.crawled_data
    outputs:
    - filtered_data: "filtered_data"
    - filtered_out_data: "filtered_out_data"
    - pass_count: "pass_count"
    - reject_count: "reject_count"
    run_mode: reactive
    depends_on: [crawler]

  # Scrape the filtered HTML data for emails, phones, links etc.
  - name: scryer
    id: scraper
    depends_on: [filter]
    input:
      crawled_data: filter.filtered_data
    config:
      extract:
        title: true
        meta: ["description", "keywords", "author", "og:title", "og:description"]
        links: true
        emails: true
        phones: true
        headers: true
        cookies: true
        custom_selectors:
          - name: "main_heading"
            selector: "h1"
          - name: "paragraphs"
            selector: "p"
      output_format: "json"
      include_failed: false
      min_text_length: 5
      max_text_length: 50000
      filters:
        content_type: ["text/html"]
        status_codes: [200]
    outputs:
    - extracted_data: "extracted_data"
    - extraction_count: "extraction_count"
    run_mode: reactive
    
  # Generate a basic report from the data
  - name: hermes
    id: reporter
    depends_on: [scraper]
    input:
      data: scraper.extracted_data
    config:
      report_title: "Aethon Web Analysis Report"
      output_dir: "reports"
    run_mode: reactive